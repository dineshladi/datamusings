{
  
    
        "post0": {
            "title": "Logistic Regression from Scratch",
            "content": "import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import normalize . Logistic Regression Equation begin{equation*} log( frac{p}{1-p}) = wx + c end{equation*} . begin{equation*} z = wx + c end{equation*} begin{equation*} sigma(z) = frac{1}{1 + e^{-z}} end{equation*} def sigmoid(z): &#39;&#39;&#39;Sigmoid function maps from a value between 0 and 1&#39;&#39;&#39; return 1/(1+np.exp(-z)) . x = np.linspace(-10,10,100) y = sigmoid(x) plt.plot(x,y) plt.title(&quot;Sigmoid Function&quot;) plt.show() . begin{equation} L(y, hat{y}) = - frac{1}{N} sum_{i=1}^N y_i log( hat{y_i}) + (1-y_i) log( hat{y_i}) end{equation} def cross_entropy_loss(w,x,y): &#39;&#39;&#39;Cross entropy loss function&#39;&#39;&#39; z = np.dot(x,w) h = sigmoid(z) total_loss = np.sum(-y*np.log(h) - (1-y)*np.log(1-h)).mean() return total_loss . Use chain rule to calculate gradient of loss . begin{equation} frac{ partial{L}}{ partial{w_i}} = frac{ partial{L}}{ partial{ hat{y}}} frac{ partial{ hat{y}}}{ partial{z}} frac{ partial{z}}{ partial{w}} end{equation}Examining each factor in turn . begin{equation} frac{ partial{L}}{ partial{ hat{y_i}}} = frac{-y_i}{ hat{y_i}} + frac{1-y_i}{1- hat{y_i}} end{equation} begin{equation} frac{ partial{L}}{ partial{ hat{y_i}}} = frac{ hat{y_i}-y_i}{ hat{y_i}(1- hat{y_i})} end{equation} begin{equation} frac{ partial{ hat{y}}}{ partial{z}} = hat{y_i}(1- hat{y_i}) end{equation} begin{equation} frac{ partial{z}}{ partial{w}} = x_i end{equation}Multiplying all the indivdual terms, you get . begin{equation} frac{ partial{L}}{ partial{w_i}} = ( hat{y_i}-y_i)x_i end{equation} def gradient(w,x,y): &#39;&#39;&#39;Gradient for cross entropy loss&#39;&#39;&#39; z = np.dot(x,w) h = sigmoid(z) gradient = np.dot(x.T,h - y) return gradient . def update_weights(learning_rate = 0.01, n_iters = 1000, loss_threshold = 0.001): ## Initialize the weights with zero values w = np.random.rand(x.shape[1]) for epoch in range(n_iters): loss = cross_entropy_loss(w,x,y) grad = gradient(w,x,y) w = w - learning_rate * grad if epoch % 1000 == 0: print(f&quot;Loss after iteration {epoch} is {round(loss,2)}&quot;) if loss &lt; loss_threshold: break return(w) . def predict(w,x): return sigmoid(np.dot(x,w)) . def accuracy(true, probs, threshold = 0.5): predicted = (probs &gt; threshold).astype(int) return 100*(predicted == true).mean() . cancer_data = load_breast_cancer() x, x_test, y, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size=0.25, random_state=0) x = normalize(x) . w = update_weights(learning_rate = 0.05, n_iters = 10000, loss_threshold = 0.001) . Loss after iteration 0 is 304.89 Loss after iteration 1000 is 88.0 Loss after iteration 2000 is 83.64 Loss after iteration 3000 is 81.82 Loss after iteration 4000 is 80.66 Loss after iteration 5000 is 79.8 Loss after iteration 6000 is 79.09 Loss after iteration 7000 is 78.49 Loss after iteration 8000 is 77.97 Loss after iteration 9000 is 77.5 . preds = predict(w,normalize(x_test)) acc = accuracy(y_test, preds) print(f&quot;Accuracy on test data is {round(acc,3)}&quot;) . Accuracy on test data is 93.706 .",
            "url": "https://dineshladi.github.io/datamusings/ml/jupyter/2020/04/24/logistic-regression.html",
            "relUrl": "/ml/jupyter/2020/04/24/logistic-regression.html",
            "date": " â€¢ Apr 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello world! This is Dinesh. I am a Data Scientist based in Bangalore,India. . Interests . Data Science/Visualization | Cricket | Cooking | Startups | . Tools . Python | R | R-Shiny | SQL | Bash | Apache Spark | . This website is powered by fastpages. .",
          "url": "https://dineshladi.github.io/datamusings/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}