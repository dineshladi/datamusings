{
  
    
        "post0": {
            "title": "Probability Distributions",
            "content": "Introduction . Random Variable is functon that maps from the world of random processes to actual number. Random Variable can be discrete or continuous. . Continuous Probability Distributions . Gaussian Distribution . Gaussian or Normal Distribution for an random variable $X$ with mean $ mu$ and variance $ sigma^2$ has the following probability density function. . $$ p(x) sim N( mu| sigma^2) $$ . $$ P(X = x) = frac{1}{ sqrt{2 pi sigma^2}} exp{ bigg[- frac{1}{2} bigg( frac{x- mu}{ sigma} bigg)^2 bigg] } $$ . #collapse-hide from scipy.stats import norm from scipy.special import factorial import numpy as np import matplotlib.pyplot as plt from matplotlib import pylab pylab.rcParams[&#39;figure.figsize&#39;] = (8.0, 6.0) for mu,var in zip([1,1,1,1],[2,4,8,20]): sigma = np.sqrt(var) x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100) y = norm.pdf(x, mu, sigma) plt.plot(x, y, lw=3, alpha=0.6, label= f&quot;$ mu = {mu}, sigma^2 = {var}$&quot;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Normal Distribution&#39;) plt.show() . . Exponential Distribution . Exponential distribution is a continuous probability distribution is characterized by parameter $ lambda$ to model time between events in a Poisson process. The probability density function for the same is . $$P(X=x) = lambda e^{- lambda x} $$ . $$ mu = frac{1}{ lambda} $$ . $$ sigma^2 = frac{1}{ lambda^2} $$ . #collapse-hide def exponential(x,rate): return rate*np.exp(-rate*x) t = np.arange(0.01, 20, 0.01) for rate in [0.25,0.5,0.75]: d = exponential(t,rate) plt.plot(t, d, linewidth=3, label = f&quot;$ lambda = {rate}$&quot;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Exponential Distribution&#39;) plt.show() . . Gamma Distribution . Gamma distribution is a continuous probability distribution characterized by parameters shape $ alpha$ and rate $ beta$. The probability density function for the same is. . $$ P(X = x) = frac{ beta^ alpha}{ Gamma( alpha)} x^{ alpha - 1} e^{- beta x } $$where $ Gamma( alpha)$ is the Gamma function. For all positive integers, $ Gamma( alpha) = ( alpha-1)! $ . $$ mu = frac{ alpha}{ beta} $$ . $$ sigma^2 = frac{ alpha}{ beta^2} $$ . #collapse-hide def Gamma(x,shape,rate): return (rate**shape)*(x**(shape-1))*np.exp(-rate*x)/factorial(shape-1) t = np.arange(0.01, 20, 0.01) for shape,rate in zip([3,3,5,10],[1,0.5,0.5,1]): d = Gamma(t,shape,rate) plt.plot(t, d, linewidth=3,label = rf&quot;$ alpha = {shape}, beta = {rate}$&quot;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Gamma Distribution&#39;) plt.show() . . Discrete Probability Distributions . Bernoulli Distribution . Bernoulli distribution is a discrete probability distribution of a single trail that takes value 1 with probability $p$ and value 0 with probability $1-p$. The probability mass function for the same is. $$ P(X = x) = p^x (1-p)^{1-x} $$ . $$ mu = p $$ . $$ sigma^2 = p(1-p) $$ . Binomial Distribution . Binomial distribution is a discrete probability distribution for number of successes in $n$ independent Bernoulli trails. Each trail has two possible outcomes either success with probability $p$ or failure with probability $1-p$. The probability mass function for the same is. $$ P(X = x) = {n choose x} p^x (1-p)^{n-x} $$ . $$ mu = np $$ . $$ sigma^2 = np(1-p) $$ . Poisson Distribution . Poisson distribution is to model discrete events that occurs at a constant rate over a given length of time. Probability mass function for the same is $$ P(X = x) = frac{ lambda^x e^{- lambda}}{x!} $$ . $$ mu = lambda $$ . $$ sigma^2 = lambda $$ . #collapse-hide def poisson(x,mu): return np.exp(-mu)*np.power(mu, x)/factorial(x) t = np.arange(0, 20, 0.01) for mu in range(2,10,2): d = poisson(t,mu) plt.plot(t, d,linewidth=3, label = f&quot;$ lambda = {mu}$&quot;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Poisson Distribution&#39;) plt.show() . . Geometric Distribution . Geometric Distribution is a discrete probability distribution of a random variables $X$ that models number of trials needed needed to get the first success for $n$ repeated independent Bernoulli trials. The probability mass function for the same is . $$ P(X=x) = (1-p)^{x-1}p $$ . $$ mu = frac{1}{p} $$ . $$ sigma^2 = frac{1-p}{p^2} $$ .",
            "url": "https://dineshladi.github.io/datamusings/probability/2020/04/26/Probability-Distributions.html",
            "relUrl": "/probability/2020/04/26/Probability-Distributions.html",
            "date": " • Apr 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic Regression from Scratch",
            "content": "Introduction . Logistic Regression is a statistical method to predict qualitative response using one or more independent variables. Instead of predicting the response directly, Logistic Regression models probability that the response belongs to a particular category. In logistic regression, we use logistic function. . The logistic function will always producr an S shaped curve, so we always get a sensible prediction. $p(X)$ is sometimes also called sigmoid function. . $$ log Big( frac{p(X)}{1-p(X)} Big) = w^TX + c $$ . $$z = w^TX + c $$ . $$ p(X) = frac{1}{1 + e^{-z}} $$ . #collapse-hide import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import normalize . . #collapse-hide def sigmoid(z): &#39;&#39;&#39;Sigmoid function maps from a value between 0 and 1&#39;&#39;&#39; return 1/(1+np.exp(-z)) x = np.linspace(-10,10,100) y = sigmoid(x) plt.plot(x,y) plt.title(&quot;Sigmoid Function&quot;) plt.show() . . Estimate Regression Coefficients . A method called maximum likelihood is to estimate the unknown coefficients. The idea behind this method is to estimate coefficients such that predicted probability $ hat{p}(x_i)$ of each observation corresponds closely as possible to the observed value of response variable for the same observation. In other words, we estimate coefficients such that $p(X)$ yields a number close to 1 for the observations with actual response value 1 and a number close to 0 for the observations with actual response value 0. The mathematical eequation for the likelihood function is . $$ ell(W) = prod_{i;y_i = 1} p(x_i) prod_{i;y_i = 0} ( 1 - p(x_i)) $$ . The $W$ estimates are chosen to maximize the likelihood function. . Manipulating the above constraint, we arrive at Cross Entropy Loss for binary classification case. begin{equation} L(y, hat{y}) = - frac{1}{N} sum_{i=1}^N y_i log( hat{y_i}) + (1-y_i) log( hat{y_i}) end{equation} . def cross_entropy_loss(w,x,y): &#39;&#39;&#39;Cross entropy loss function&#39;&#39;&#39; z = np.dot(x,w) h = sigmoid(z) total_loss = np.sum(-y*np.log(h) - (1-y)*np.log(1-h)).mean() return total_loss . Use chain rule to calculate gradient of loss . begin{equation} frac{ partial{L}}{ partial{w_i}} = frac{ partial{L}}{ partial{ hat{y}}} frac{ partial{ hat{y}}}{ partial{z}} frac{ partial{z}}{ partial{w}} end{equation}Examining each factor in turn . begin{equation} frac{ partial{L}}{ partial{ hat{y_i}}} = frac{-y_i}{ hat{y_i}} + frac{1-y_i}{1- hat{y_i}} end{equation} begin{equation} frac{ partial{L}}{ partial{ hat{y_i}}} = frac{ hat{y_i}-y_i}{ hat{y_i}(1- hat{y_i})} end{equation} begin{equation} frac{ partial{ hat{y}}}{ partial{z}} = hat{y_i}(1- hat{y_i}) end{equation} begin{equation} frac{ partial{z}}{ partial{w}} = x_i end{equation}Multiplying all the indivdual terms, you get . begin{equation} frac{ partial{L}}{ partial{w_i}} = ( hat{y_i}-y_i)x_i end{equation} def gradient(w,x,y): &#39;&#39;&#39;Gradient for cross entropy loss&#39;&#39;&#39; z = np.dot(x,w) h = sigmoid(z) gradient = np.dot(x.T,h - y) return gradient . We are gonnna use a algorithm called batch gradient descent to find the optimal weights. . $$ Delta w_{i} = - eta frac{ partial L}{ partial w_{i}} $$ . $$ w_{i} := w_{i} + Delta w_{i} $$ . def update_weights(learning_rate = 0.01, n_iters = 1000, loss_threshold = 0.001): ## Initialize the weights with zero values w = np.random.rand(x.shape[1]) for epoch in range(n_iters): loss = cross_entropy_loss(w,x,y) grad = gradient(w,x,y) w = w - learning_rate * grad if epoch % 1000 == 0: print(f&quot;Loss after iteration {epoch} is {round(loss,2)}&quot;) if loss &lt; loss_threshold: break return(w) . def accuracy(true, probs, threshold = 0.5): predicted = (probs &gt; threshold).astype(int) return 100*(predicted == true).mean() def predict(w,x): return sigmoid(np.dot(x,w)) . Lets load some binary classification data from sklearn.datasets and split the data into train test split. . cancer_data = load_breast_cancer() x, x_test, y, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size=0.25, random_state=0) x = normalize(x) . Lets call the update weights function to find the optimal weights that minimizes cross entropy loss . w = update_weights(learning_rate = 0.05, n_iters = 10000, loss_threshold = 0.001) . Loss after iteration 0 is 281.55 Loss after iteration 1000 is 87.93 Loss after iteration 2000 is 83.59 Loss after iteration 3000 is 81.78 Loss after iteration 4000 is 80.63 Loss after iteration 5000 is 79.76 Loss after iteration 6000 is 79.06 Loss after iteration 7000 is 78.46 Loss after iteration 8000 is 77.94 Loss after iteration 9000 is 77.47 . Lets use the model to generate predictions for the test data and see what kind of accuracies are we reaching. . preds = predict(w,normalize(x_test)) acc = accuracy(y_test, preds) print(f&quot;Accuracy on test data is {round(acc,3)}&quot;) . Accuracy on test data is 93.706 .",
            "url": "https://dineshladi.github.io/datamusings/ml/jupyter/2020/04/24/logistic-regression.html",
            "relUrl": "/ml/jupyter/2020/04/24/logistic-regression.html",
            "date": " • Apr 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello world! This is Dinesh. I am a Data Scientist based in Bangalore,India. . Interests . Data Science/Visualization | Cricket | Cooking | Startups | . Tools . Python | R | R-Shiny | SQL | Bash | Apache Spark | . This website is powered by fastpages. .",
          "url": "https://dineshladi.github.io/datamusings/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}